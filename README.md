Introduction:
This repository provides a Python-based framework to prioritize metamorphic relations (MRs) for evaluating large language models (LLMs), especially in fairness testing scenarios. The approach uses diversity-based metrics to quantify the behavioral spread of test cases generated by different MRs, helping researchers and developers select the most impactful MRs during model evaluation. The research paper of this work can be accessed here: https://arxiv.org/pdf/2505.07870?


Steps to Prioritize MRs for LLMs:

1) Run run_metrics.py file in the root folder of your project. The code will run the metrics and generate results folder for individual metrics.
2) Run Total_diversity_score_calculation.py file and it will give the diversity score.
3) Follow step1 and step2 for all the MRs and calculate the diversity score.
4) Rank the MRs based on diversity score and use the ranked MRs when testing your LLM.

Citation

If you use this work in your research or application, please cite:
@misc{mr-prioritization-llm,
  author = {Srinivasan, Madhusudan},
  title = {Prioritizing Metamorphic Relations for Fairness Testing of Large Language Models},
  year = {2025},
  howpublished = {\url{https://github.com/ST-Lab-Srinivasan/MR-Prioritize-LLM}},
  note = {GitHub repository}
}

